{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and cleaning\n",
    "\n",
    "Our team was unable to find a dataset that contained pricing data for the S&P 500 over the last 10 years. Therefore, in order to gather this data into a workable dataset, we utilized a list of the S&P 500 companies along with the Yahoo Finance API to create our own dataset.\n",
    "\n",
    "Source data:\n",
    "- Data of current S&P 500 Company from https://github.com/datasets/s-and-p-500-companies/blob/master/data/constituents.csv\n",
    "- Yahoo Finance API: http://theautomatic.net/yahoo_fin-documentation/\n",
    "\n",
    "These files can be downloaded in the Google Drive folder https://drive.google.com/drive/folders/1I_hP_G51eLKYcYOwQKrcTGOimkLhTL52?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert data from S&P 500 Companies list into text file with tickers of each company\n",
    "spList = pd.read_csv('Source Data/constituents.csv')\n",
    "spListTick = spList[\"Symbol\"].tolist()\n",
    "spListTick.append('SPY') # adding SPY into list\n",
    "textfile = open(\"companies.txt\", \"w\")\n",
    "for element in spListTick:\n",
    "    textfile.write(element + \" \")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We first downloaded a csv file on the current S&P 500 companies and their associated sectors from https://github.com/datasets/s-and-p-500-companies/blob/master/data/constituents.csv. We converted the list of companies to a txt file to be used to later call the Yahoo Finance API. We also wrote the S&P 500 Index into the list as well (with the ticker being SPY) in order to gain some more data on the overall market's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert data from S&P 500 Companies list into text file with tickers of each company\n",
    "spList = pd.read_csv('Source Data/constituents.csv')\n",
    "spListTick = spList[\"Symbol\"].tolist()\n",
    "spListTick.append('SPY') # adding SPY into list\n",
    "textfile = open(\"companies.txt\", \"w\")\n",
    "for element in spListTick:\n",
    "    textfile.write(element + \" \")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install yahoo finance API\n",
    "import sys\n",
    "!pip3 install yahoo-fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. \n",
    "import yahoo_fin.stock_info as si\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"companies.txt\") as file:\n",
    "    lines = file.readlines()\n",
    "    companyList = [line.rstrip() for line in lines]\n",
    "stockData = {}\n",
    "sum_df=pd.DataFrame()\n",
    "for ticker in companyList:\n",
    "    try:\n",
    "        stockData[ticker] = si.get_data(ticker, start_date=\"01/01/2010\", end_date=\"01/03/2020\", interval=\"1mo\")\n",
    "        sum_df = sum_df.append(stockData[ticker])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "sum_df.index=sum_df.index.rename(\"data\")\n",
    "sum_df.to_excel(\"output.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_excel(\"output.xlsx\",index_col=None)\n",
    "output.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['monthly_return'] = output.groupby(['ticker'])['adjclose'].apply(pd.Series.pct_change)*100\n",
    "output.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the company's sector into the final dataset as a new column by merging the constituents.csv file with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spList = spList.rename(columns={'Symbol': 'ticker', 'Sector': 'sector', \"Name\": 'name'})\n",
    "df2 = pd.DataFrame({'name': [\"S&P 500\"],'ticker': ['SPY'], 'sector': ['All']})\n",
    "spList = spList.append(df2, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalOutput = pd.merge(output, spList, on=\"ticker\")\n",
    "\n",
    "\n",
    "check = (output.shape[0] == finalOutput.shape[0])\n",
    "\n",
    "print(\"Was the join performed correctly? {}\".format(check))\n",
    "\n",
    "\n",
    "print(finalOutput.shape[0])\n",
    "print(output.shape[0])\n",
    "\n",
    "finalOutput.tail()\n",
    "finalOutput.to_csv('finalOutput.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create two new columns: one to track if an observation is above the average monthly return for a company and another to track if the previous month's return was higher than the average monthly return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputNew = pd.read_csv('finalOutput.csv')\n",
    "outputNew['month']=pd.DatetimeIndex(outputNew['date']).month\n",
    "outputNew.head()\n",
    "tickers = outputNew.ticker.unique()\n",
    "averages = outputNew.groupby(['ticker']).monthly_return.mean()\n",
    "for i in tickers:\n",
    "    outputNew['averages'] = averages[i]\n",
    "    \n",
    "def isAboveAverage(averages,monthlyret):\n",
    "    if monthlyret > averages:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "outputNew[\"above_avg\"] = outputNew.apply(lambda x: isAboveAverage(x[\"averages\"],x['monthly_return']), axis=1)\n",
    "\n",
    "outputNew.tail()\n",
    "\n",
    "def previous_above_avg(dataframe):\n",
    "    i=0\n",
    "    while i < dataframe.shape[0]:\n",
    "        startingTicker=dataframe.iloc[i][\"ticker\"]\n",
    "        i+=1\n",
    "        #Keep going until change in ticker or year\n",
    "        while i < dataframe.shape[0] and dataframe.iloc[i][\"ticker\"] == startingTicker:\n",
    "            dataframe.at[i,\"prev_above_avg\"] =  dataframe.at[i-1,\"above_avg\"]\n",
    "            i+=1\n",
    "    return dataframe\n",
    "\n",
    "outputNew[\"prev_above_avg\"]=None\n",
    "outputNew=previous_above_avg(outputNew)\n",
    "outputNew.to_csv('finalOutput.csv', index=False)\n",
    "outputNew.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
